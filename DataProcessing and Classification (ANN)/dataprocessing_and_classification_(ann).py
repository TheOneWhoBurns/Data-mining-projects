# -*- coding: utf-8 -*-
"""DataProcessing and Classification (ANN).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIdxCiTmoaotdUxAw_Kvv6R8VvJQ909g
"""

import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, label_binarize
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score, f1_score, matthews_corrcoef, confusion_matrix, roc_curve, precision_recall_curve,auc, log_loss
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel, mutual_info_classif
from sklearn.base import clone
from scipy.stats import entropy
from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from matplotlib import pyplot as plt
from sklearn.impute import SimpleImputer
import seaborn as sns
from sklearn.model_selection import StratifiedKFold
import re
import warnings
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
warnings.filterwarnings('ignore')


# Función para cargar y preprocesar los datos
def load_and_preprocess_data():
    dataset = pd.read_csv('https://raw.githubusercontent.com/Pamela2710/Data-Mining/main/DataProcessing/features_completas.csv', encoding='latin-1')
    dataset.dropna(inplace=True)
    dataset.drop(index=dataset[dataset['Etiqueta']=='Tremor'].index, inplace=True)
    dataset.drop(columns=['espectrograma etiqueta'], inplace=True)  # Eliminar columna no relevante

    label_encoder = LabelEncoder()
    dataset['Etiqueta'] = label_encoder.fit_transform(dataset['Etiqueta'])

    X = dataset.iloc[:, :-1]
    y = dataset['Etiqueta']

    return pd.DataFrame(X, columns=X.columns), y


x, y = load_and_preprocess_data()

scaler = MinMaxScaler()

# Normalización
x_scaled = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)

a = [ [0, 4, 6, 13, 14, 18, 20, 22, 23, 26, 30, 34, 37, 41, 45, 48, 49, 50, 51, 54, 57, 58, 62, 66, 69, 74, 75, 76, 77, 78, 81, 82, 83, 85, 86, 89, 92, 93, 94, 95, 99, 102, 103, 106, 107],
      [3, 8, 12, 13, 15, 17, 19, 23, 24, 25, 26, 27, 29, 32, 35, 36, 38, 41, 42, 44, 47, 48, 50, 51, 54, 55, 57, 59, 64, 68, 69, 73, 77, 78, 79, 81, 82, 85, 86, 89, 90, 91, 98, 100, 102, 103, 104, 106],
      [2, 4, 5, 6, 12, 15, 16, 20, 22, 23, 24, 25, 26, 27, 29, 30, 31, 33, 35, 36, 40, 46, 49, 50, 51, 53, 57, 58, 61, 62, 64, 69, 71, 72, 75, 76, 78, 79, 81, 86, 89, 90, 91, 95, 97, 99, 100, 101, 102, 103, 104, 106]]

# Obtener características y etiquetas

X_list = []
for indices in a:
    reduced = x_scaled.iloc[:, indices]
    X_list.append(reduced)

def remove_zero_columns(df):
  return df.loc[:, (df != 0).any(axis=0)]
x_scaled = remove_zero_columns(x_scaled)

def stratified_kfold_cv(X, y, n_splits=10):
    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    for train_index, val_index in stratified_kfold.split(X, y):
        X_train, y_train = X.iloc[train_index], y.iloc[train_index]  # Usar .iloc para acceder a las filas por índice
        X_val, y_val = X.iloc[val_index], y.iloc[val_index]

        yield X_train, y_train, X_val, y_val

warnings.filterwarnings("ignore")

x_scaled.describe().transpose()

"""Investigar topologias

Simple MLP
"""

# Configuración de validación cruzada estratificada
stratified_kfold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Definir hiperparámetros
learning_rates = [0.1, 0.3, 0.5]
epochs = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]

# Listas para almacenar métricas
accuracy_set = []
precision_set = []
recall_set = []
auc_set = []
loss_set = []
f1_set=[]
accuracy_std = []
precision_std = []
recall_std = []
f1_std=[]
auc_std = []
loss_std = []
confusion_matrix_set = []

roc_auc_plot = []
fpr_plot = []
tpr_plot = []
precision_plot = []
recall_plot = []
loss_plot = []

# Neural Network
for learning_rate in learning_rates:
  print(f"\nLearning Rate: {learning_rate}:")
  for X in X_list:
      acc_epochs = []
      pres_epochs = []
      rec_epochs = []
      f1_epochs = []
      auc_epochs = []
      loss_epochs = []
      confusion_matrix_epochs = []

      for epoch in epochs:
          accuracy_epoch = []
          precision_epoch = []
          recall_epoch = []
          auc_epoch = []
          f1_epoch = []
          loss_epoch = []

          for fold, (train_index, val_index) in enumerate(stratified_kfold_cv.split(X.values, y)):
              X_train, X_val = X.values[train_index], X.values[val_index]
              y_train, y_val = y.values[train_index], y.values[val_index]


              mlp = MLPClassifier(hidden_layer_sizes=(15, 15), activation='relu',learning_rate_init=learning_rate, max_iter=epoch, random_state=42)
              mlp.fit(X_train, y_train)
              y_val_pred = mlp.predict(X_val)
              y_proba = mlp.predict_proba(X_val)


              accuracy_val = accuracy_score(y_val, y_val_pred)
              accuracy_epoch.append(accuracy_val)

              precision_val = precision_score(y_val, y_val_pred, average='weighted')
              precision_epoch.append(precision_val)

              recall_val = recall_score(y_val, y_val_pred, average='weighted')
              recall_epoch.append(recall_val)

              auc_val = roc_auc_score(y_val, y_proba, average='weighted', multi_class='ovr')
              auc_epoch.append(auc_val)

              f1_val=f1_score(y_val, y_val_pred, average='weighted')
              f1_epoch.append(f1_val)

              loss_val = log_loss(y_val, y_proba)
              loss_epoch.append(loss_val)

              confusion_matrix_val = confusion_matrix(y_val, y_val_pred)
              if fold == 0:
                  confusion_matrix_epochs = confusion_matrix_val
              else:
                  confusion_matrix_epochs += confusion_matrix_val

          avg_accuracy_epoch = np.mean(accuracy_epoch)
          acc_epochs.append(avg_accuracy_epoch)

          avg_precision_epoch = np.mean(precision_epoch)
          pres_epochs.append(avg_precision_epoch)

          avg_recall_epoch = np.mean(recall_epoch)
          rec_epochs.append(avg_recall_epoch)

          avg_f1_epoch = np.mean(f1_epoch)
          f1_epochs.append(avg_f1_epoch)

          avg_auc_epoch = np.mean(auc_epoch)
          auc_epochs.append(avg_auc_epoch)

          avg_loss_epoch = np.mean(loss_epoch)
          loss_epochs.append(avg_loss_epoch)

      accuracy_set.append(acc_epochs)
      precision_set.append(pres_epochs)
      recall_set.append(rec_epochs)
      f1_set.append(f1_epochs)
      auc_set.append(auc_epochs)
      loss_set.append(loss_epochs)

      accuracy_std.append(np.std(acc_epochs))
      precision_std.append(np.std(pres_epochs))
      recall_std.append(np.std(rec_epochs))
      f1_std.append(np.std(f1_epochs))
      auc_std.append(np.std(auc_epochs))
      loss_std.append(np.std(loss_epochs))

      confusion_matrix_set.append(confusion_matrix_epochs)

      fpr_plot = []
      tpr_plot = []
      n_classes = 2

      fpr_fold, tpr_fold, _ = roc_curve(label_binarize(y_val, classes=np.unique(y))[:, 0], y_proba[:, 0])
      fpr_plot.append(fpr_fold)
      tpr_plot.append(tpr_fold)

      precision_fold, recall_fold, _ = precision_recall_curve(label_binarize(y_val, classes=np.unique(y))[:, 0], y_proba[:, 0])
      precision_plot.append(precision_fold)
      recall_plot.append(recall_fold)

      loss_plot.append(loss_epochs)


  # Imprimir resultados
  for i, (acc, pres, rec, f1, auc, loss) in enumerate(zip(accuracy_set, precision_set, recall_set, f1_set, auc_set, loss_set)):
      print(f"\nConjunto de datos {i + 1}:")

      print("Epochs\tAccuracy\tPrecision\tRecall\tF1\tAUC\tLoss")

      for j, (acc_epoch, pres_epoch, rec_epoch,f1_epoch, auc_epoch, loss_epoch) in enumerate(zip(acc, pres, rec,f1, auc, loss)):
          print(f"{epochs[j]}\t{acc_epoch:.4f}\t\t{pres_epoch:.4f}\t\t{rec_epoch:.4f}\t{f1_epoch:.4f}\t{auc_epoch:.4f}\t{loss_epoch:.4f}")

      print("---------------------------------")
      print(f"Average:\t{np.mean(acc):.4f}\t\t{np.mean(pres):.4f}\t\t{np.mean(rec):.4f}\t\t{np.mean(f1):.4f}\t{np.mean(auc):.4f}\t{np.mean(loss):.4f}")
      print(f"Std Deviation:\t{accuracy_std[i]:.4f}\t\t{precision_std[i]:.4f}\t\t{recall_std[i]:.4f}\t\t{f1_std[i]:.4f}\t{auc_std[i]:.4f}\t{loss_std[i]:.4f}")
      print("Confusion Matrix:")
      print(confusion_matrix_set[i])

"""# Fully Conected ANN
Una ann simple, \\
sigmoid activation, \\
softmax loss, \\
FFBP
"""

import numpy as np
from scipy.special import softmax

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, x * alpha)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)

def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    epsilon = 1e-6  # A small value to prevent log(0)
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Ensure y_pred does not hit exactly 0 or 1
    cross_entropy = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)
    loss = alpha * np.power(1 - y_pred, gamma) * cross_entropy + (1 - alpha) * np.power(y_pred, gamma) * cross_entropy
    return np.mean(np.sum(loss, axis=1))

def focal_loss_derivative(y_true, y_pred, gamma=2.0, alpha=0.25):
    epsilon = 1e-6  # A small value to prevent division by zero or log(0)
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Ensure y_pred does not hit exactly 0 or 1
    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)
    gamma_factor = y_true * gamma * (1 - y_pred) + (1 - y_true) * gamma * y_pred
    derivative = alpha_factor * gamma_factor * (p_t ** (gamma - 1)) * (y_true - y_pred) / p_t
    return derivative


def softmax_derivative(x):
    s = softmax(x, axis=1)
    return s * (1 - s)

class FullyConnectedNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, lambda_l1=0.0, lambda_l2=0.0):
        self.weights_input_hidden = np.random.randn(input_size, hidden_size) * 0.01
        self.weights_hidden_output = np.random.randn(hidden_size, output_size) * 0.01
        self.bias_hidden = np.zeros(hidden_size)
        self.bias_output = np.zeros(output_size)
        self.lambda_l1 = lambda_l1
        self.lambda_l2 = lambda_l2
        # Batch normalization parameters
        self.gamma_hidden = np.ones(hidden_size)
        self.beta_hidden = np.zeros(hidden_size)

    def batch_norm_forward(self, X, gamma, beta):
        mu = np.mean(X, axis=0)
        var = np.var(X, axis=0)
        X_norm = (X - mu) / np.sqrt(var + 1e-8)  # Adding a small value to avoid division by zero
        out = gamma * X_norm + beta
        return out, X_norm, mu, var

    def batch_norm_backward(self, dout, cache):
        X_norm, gamma, mu, var = cache
        N, D = dout.shape

        X_mu = X_norm * np.sqrt(var + 1e-8)
        std_inv = 1. / np.sqrt(var + 1e-8)

        dX_norm = dout * gamma
        dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3
        dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)

        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)
        dgamma = np.sum(dout * X_norm, axis=0)
        dbeta = np.sum(dout, axis=0)

        return dX, dgamma, dbeta

    def forward(self, X):
        self.Z1 = np.dot(X, self.weights_input_hidden) + self.bias_hidden
        self.BN1, self.norm1, self.mu1, self.var1 = self.batch_norm_forward(self.Z1, self.gamma_hidden, self.beta_hidden)
        self.hidden = leaky_relu(self.BN1)
        self.Z2 = np.dot(self.hidden, self.weights_hidden_output) + self.bias_output
        self.output = softmax(self.Z2, axis=1)
        return self.output

    def backpropagate(self, X, y, output, learning_rate):
        d_output = focal_loss_derivative(y, output) * softmax_derivative(output)
        d_hidden_layer = d_output.dot(self.weights_hidden_output.T) * leaky_relu_derivative(self.hidden)

        # Regularization terms
        l1_input_hidden = self.lambda_l1 * np.sign(self.weights_input_hidden)
        l1_hidden_output = self.lambda_l1 * np.sign(self.weights_hidden_output)
        l2_input_hidden = self.lambda_l2 * self.weights_input_hidden
        l2_hidden_output = self.lambda_l2 * self.weights_hidden_output

        # Update weights with regularization
        self.weights_hidden_output += (self.hidden.T.dot(d_output) - l1_hidden_output - l2_hidden_output) * learning_rate
        self.weights_input_hidden += (X.T.dot(d_hidden_layer) - l1_input_hidden - l2_input_hidden) * learning_rate

    def train(self, X, y, iterations, learning_rate):
        for i in range(iterations):
            output = self.forward(X)
            loss = focal_loss(y, output)  # Using focal loss
            self.backpropagate(X, y, output, learning_rate)
            if i % 10 == 0:  # Print the loss every 10 iterations
                print(f"Iteration {i+1}/{iterations}, Loss: {loss}")

from sklearn.metrics import roc_auc_score

input_size = x_scaled.shape[1]
output_size = np.unique(y).size
hidden_size = 5
learning_rates = np.arange(0.1, 0.5 + 0.1, 0.2)
epochs_options = range(1, 200 + 1, 20)

best_auc = 0
best_lr = 0
best_epoch = 0
best_hidden_size = 0

for learning_rate in learning_rates:
    for epochs in epochs_options:
        fold_auc = []  # Store AUC for each fold
        for X_train, y_train, X_val, y_val in stratified_kfold_cv(pd.DataFrame(x_scaled), y):
            nn = FullyConnectedNeuralNetwork(input_size, hidden_size, output_size)
            X_train_np = X_train.to_numpy()
            y_train_np = np.eye(output_size)[y_train]  # Convert labels to one-hot encoding

            nn.train(X_train_np, y_train_np, epochs, learning_rate)

            X_val_np = X_val.to_numpy()
            predictions = nn.forward(X_val_np)

            # Skip the loop if predictions contain NaN
            if np.isnan(predictions).any():
                print("Skipping due to NaN in predictions")
                continue

            # For multi-class scenarios, we calculate the ROC AUC score considering the OvR approach
            # This requires the true labels to not be in one-hot encoding but in their original form
            if output_size > 2:
                roc_auc = roc_auc_score(y_val,
                                        predictions,
                                        multi_class="ovr")
            else:
                # For binary classification, `y_score` can be probability estimates of the positive class
                roc_auc = roc_auc_score(y_val, predictions[:, 1])

            fold_auc.append(roc_auc)

        avg_auc = np.mean(fold_auc)
        if avg_auc > best_auc:
            best_auc = avg_auc
            best_lr = learning_rate
            best_epoch = epochs
            best_hidden_size = hidden_size

print(f"Best ROC AUC: {best_auc} achieved with learning rate: {best_lr}, epochs: {best_epoch}, and hidden size: {best_hidden_size}")

"""# Sparse ANN
Una Ann simple pero con pruning para valores de menos de 0.1
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def initialize_sparse_weights(size, sparsity_level):
    """Initialize weights with a given sparsity level."""
    assert 0 <= sparsity_level <= 1, "Sparsity level must be between 0 and 1."
    total_elements = size[0] * size[1]
    num_zeros = int(total_elements * sparsity_level)
    weights = np.random.randn(*size).flatten()
    zero_indices = np.random.choice(range(total_elements), num_zeros, replace=False)
    weights[zero_indices] = 0
    return weights.reshape(size)

class SparseNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, sparsity_level=0.5):
        # Initialize weights with some sparsity
        self.weights_input_hidden = initialize_sparse_weights((input_size, hidden_size), sparsity_level)
        self.weights_hidden_output = initialize_sparse_weights((hidden_size, output_size), sparsity_level)

    def forward(self, X):
        self.hidden = sigmoid(np.dot(X, self.weights_input_hidden))
        output = sigmoid(np.dot(self.hidden, self.weights_hidden_output))
        return output

    def backpropagate(self, X, y, output, learning_rate):
        error = y - output
        # Calculate deltas
        d_output = error * sigmoid_derivative(output)
        d_hidden_layer = d_output.dot(self.weights_hidden_output.T) * sigmoid_derivative(self.hidden)

        self.weights_hidden_output += self.hidden.T.dot(d_output) * learning_rate
        self.weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate

    def prune_weights(self, threshold=0.1):
        """Zero out small weights."""
        self.weights_input_hidden[self.weights_input_hidden < threshold] = 0
        self.weights_hidden_output[self.weights_hidden_output < threshold] = 0

    def train(self, X, y, iterations, learning_rate, prune_every_n=10):
        for i in range(iterations):
            output = self.forward(X)
            self.backpropagate(X, y, output, learning_rate)
            if (i + 1) % prune_every_n == 0:
                self.prune_weights()

# Assuming `x_scaled` and `y` are already defined and preprocessed for multi-class classification
input_size = x_scaled.shape[1]
hidden_size = 5  # Adjust based on experimentation
output_size = np.unique(y).size  # Adjust for the number of classes

learning_rates = np.arange(0.1, 0.5 + 0.1, 0.1)
epochs_options = range(1, 200 + 1, 20)

best_auc = 0
best_lr = 0
best_epoch = 0

for learning_rate in learning_rates:
    for epochs in epochs_options:
        fold_auc = []  # Store AUC for each fold
        fold = 1
        for X_train, y_train, X_val, y_val in stratified_kfold_cv(pd.DataFrame(x_scaled), y):
            nn = SparseNeuralNetwork(input_size, hidden_size, output_size)
            X_train_np = X_train.to_numpy()
            y_train_np = np.eye(output_size)[y_train]  # Convert labels to one-hot encoding

            nn.train(X_train_np, y_train_np, epochs, learning_rate)

            X_val_np = X_val.to_numpy()
            predictions = nn.forward(X_val_np)
            auc = roc_auc_score(np.eye(output_size)[y_val], predictions, multi_class='ovr')  # One-vs-Rest
            fold_auc.append(auc)
            fold += 1

        avg_auc = np.mean(fold_auc)
        if avg_auc > best_auc:
            best_auc = avg_auc
            best_lr = learning_rate
            best_epoch = epochs

print(f"Best AUC: {best_auc} achieved with learning rate: {best_lr} and epochs: {best_epoch}")